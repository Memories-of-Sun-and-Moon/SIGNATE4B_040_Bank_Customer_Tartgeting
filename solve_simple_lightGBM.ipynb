{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import japanize_matplotlib\n",
    "import lightgbm as lgb\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import KFold\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv('data/train.csv')\n",
    "df_test = pd.read_csv('data/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.concat([df_train, df_test], sort=False)\n",
    "\n",
    "\n",
    "# no yes を 0 と 1 に変更\n",
    "data['default'].replace(['no', 'yes'], [0, 1], inplace=True)\n",
    "data['housing'].replace(['no', 'yes'], [0, 1], inplace=True)\n",
    "data['loan'].replace(['no', 'yes'], [0, 1], inplace=True)\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "job_le = LabelEncoder()\n",
    "data['job'] = job_le.fit_transform(data['job'])\n",
    "marital_le = LabelEncoder()\n",
    "data['marital'] = marital_le.fit_transform(data['marital'])\n",
    "education_le = LabelEncoder()\n",
    "data['education'] = education_le.fit_transform(data['education'])\n",
    "housing_le = LabelEncoder()\n",
    "data['housing'] = housing_le.fit_transform(data['housing'])\n",
    "loan_le = LabelEncoder()\n",
    "data['loan'] = loan_le.fit_transform(data['loan'])\n",
    "contact_le = LabelEncoder()\n",
    "data['contact'] = contact_le.fit_transform(data['contact'])\n",
    "poutcome_le = LabelEncoder()\n",
    "data['poutcome'] = poutcome_le.fit_transform(data['poutcome'])\n",
    "\n",
    "\n",
    "data['month'] = data['month'].map({'jan':1, 'feb':2, 'mar':3, 'apr':4, 'may':5, 'jun':6, 'jul':7, 'aug':8, 'sep':9, 'oct':10, 'nov':11, 'dec':12}).astype(int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_features = ['job', 'marital', 'education', 'default', 'housing', 'loan', 'previous']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   id  age  job  marital  education  default  balance  housing  loan  contact  \\\n",
      "0   0   31    7        1          1        0    12294        1     0        0   \n",
      "1   1   29    2        2          2        0    43027        0     0        0   \n",
      "2   2   35    4        1          2        0    12252        1     0        0   \n",
      "3   3   31    9        1          1        0    99121        1     1        2   \n",
      "4   4   48   10        1          0        0    42005        1     0        1   \n",
      "\n",
      "   day  month  duration  campaign  pdays  previous  poutcome  \n",
      "0   21     11       101         3    498         0         1  \n",
      "1   22      8       158         2    702         0         3  \n",
      "2   11     11       351         1    826         0         0  \n",
      "3   16      5       658         2    120         0         0  \n",
      "4    3      4       177         1    273         0         3  \n"
     ]
    }
   ],
   "source": [
    "train = data[:len(df_train)]\n",
    "test = data[len(df_train):]\n",
    "\n",
    "y_train = train['y'].astype(int)\n",
    "X_train = train.drop('y', axis=1)\n",
    "X_test = test.drop('y', axis=1)\n",
    "\n",
    "print(X_train.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_valid, y_train, y_valid = train_test_split(X_train, y_train, test_size=0.3, random_state=0, stratify=y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 1178, number of negative: 13998\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.042773 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 982\n",
      "[LightGBM] [Info] Number of data points in the train set: 15176, number of used features: 16\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.077623 -> initscore=-2.475096\n",
      "[LightGBM] [Info] Start training from score -2.475096\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "Early stopping, best iteration is:\n",
      "[66]\ttraining's binary_logloss: 0.144081\tvalid_1's binary_logloss: 0.194435\n",
      "fold 0, AUC: 0.8595163136805953\n",
      "[LightGBM] [Info] Number of positive: 1178, number of negative: 13998\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.003137 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 982\n",
      "[LightGBM] [Info] Number of data points in the train set: 15176, number of used features: 16\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.077623 -> initscore=-2.475096\n",
      "[LightGBM] [Info] Start training from score -2.475096\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "Early stopping, best iteration is:\n",
      "[62]\ttraining's binary_logloss: 0.144041\tvalid_1's binary_logloss: 0.205804\n",
      "fold 1, AUC: 0.8553873306620874\n",
      "[LightGBM] [Info] Number of positive: 1200, number of negative: 13976\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.003809 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 983\n",
      "[LightGBM] [Info] Number of data points in the train set: 15176, number of used features: 16\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.079072 -> initscore=-2.455020\n",
      "[LightGBM] [Info] Start training from score -2.455020\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "Early stopping, best iteration is:\n",
      "[52]\ttraining's binary_logloss: 0.15439\tvalid_1's binary_logloss: 0.200333\n",
      "fold 2, AUC: 0.8394441443432284\n",
      "[LightGBM] [Info] Number of positive: 1191, number of negative: 13985\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001216 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 984\n",
      "[LightGBM] [Info] Number of data points in the train set: 15176, number of used features: 16\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.078479 -> initscore=-2.463192\n",
      "[LightGBM] [Info] Start training from score -2.463192\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "Early stopping, best iteration is:\n",
      "[42]\ttraining's binary_logloss: 0.16256\tvalid_1's binary_logloss: 0.201564\n",
      "fold 3, AUC: 0.8414142347460382\n",
      "[LightGBM] [Info] Number of positive: 1165, number of negative: 14011\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000348 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 982\n",
      "[LightGBM] [Info] Number of data points in the train set: 15176, number of used features: 16\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.076766 -> initscore=-2.487122\n",
      "[LightGBM] [Info] Start training from score -2.487122\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "Early stopping, best iteration is:\n",
      "[41]\ttraining's binary_logloss: 0.157943\tvalid_1's binary_logloss: 0.220236\n",
      "fold 4, AUC: 0.8303533650955942\n",
      "CV score: 0.8452230777055088\n"
     ]
    }
   ],
   "source": [
    "FOLD = 5\n",
    "NUM_ROUND = 1000\n",
    "\n",
    "\n",
    "params = {\n",
    "    'objective': 'binary', \n",
    "}\n",
    "valid_scores = []\n",
    "models = []\n",
    "\n",
    "kf = KFold(n_splits=FOLD, shuffle=True, random_state=0)\n",
    "\n",
    "for fold, (train_indices, valid_indices) in enumerate(kf.split(X_train)):\n",
    "    X_t, X_v = X_train.iloc[train_indices], X_train.iloc[valid_indices]\n",
    "    y_t, y_v = y_train.iloc[train_indices], y_train.iloc[valid_indices]\n",
    "\n",
    "    lgb_train = lgb.Dataset(X_t, y_t, categorical_feature=categorical_features)\n",
    "    lgb_eval = lgb.Dataset(X_v, y_v, categorical_feature=categorical_features)\n",
    "    \n",
    "    model = lgb.train(params, lgb_train, valid_sets=[lgb_train, lgb_eval], num_boost_round=1000, callbacks=[lgb.early_stopping(stopping_rounds=10, verbose=True), lgb.log_evaluation(0)])\n",
    "\n",
    "    y_valid_pred = model.predict(X_v, num_iteration=model.best_iteration)\n",
    "\n",
    "    score = roc_auc_score(y_v, y_valid_pred)\n",
    "\n",
    "    print(f'fold {fold}, AUC: {score}')\n",
    "    valid_scores.append(score)\n",
    "    models.append(model)\n",
    "\n",
    "cv_score = np.mean(valid_scores)\n",
    "print(f'CV score: {cv_score}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 1478, number of negative: 17492\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001685 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 987\n",
      "[LightGBM] [Info] Number of data points in the train set: 18970, number of used features: 16\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.077912 -> initscore=-2.471054\n",
      "[LightGBM] [Info] Start training from score -2.471054\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "Early stopping, best iteration is:\n",
      "[46]\ttraining's binary_logloss: 0.165099\tvalid_1's binary_logloss: 0.20454\n"
     ]
    }
   ],
   "source": [
    "lgb_train = lgb.Dataset(X_train, y_train, categorical_feature=categorical_features)\n",
    "lgb_eval = lgb.Dataset(X_valid, y_valid, categorical_feature=categorical_features)\n",
    "\n",
    "model = lgb.train(params, lgb_train, valid_sets=[lgb_train, lgb_eval], num_boost_round=1000, callbacks=[lgb.early_stopping(stopping_rounds=10, verbose=True), lgb.log_evaluation(0)])\n",
    "\n",
    "y_pred = model.predict(X_test, num_iteration=model.best_iteration)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
